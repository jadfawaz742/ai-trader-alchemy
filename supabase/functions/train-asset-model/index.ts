import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.39.3';
import { fetchMarketData as fetchUnifiedData, type MarketDataPoint } from '../_shared/market-data-fetcher.ts';
import { TrainingRequestSchema, validateInput, createValidationErrorResponse } from '../_shared/validation-schemas.ts';
import { initializeModel, forwardPass, serializeModel } from '../_shared/recurrent-ppo-model.ts';
import { TradingEnvironment } from '../_shared/trading-environment.ts';
import { PPOTrainer, ExperienceBuffer } from '../_shared/ppo-trainer.ts';
import { extractStructuralFeatures } from '../_shared/structural-features.ts';
import { calculateTechnicalIndicators } from '../_shared/technical-indicators.ts';

// Force redeploy to pick up 25-feature extraction fix in trading-environment.ts
const FEATURE_FIX_VERSION = '2.0.0';
console.log(`üîß train-asset-model v${FEATURE_FIX_VERSION} - 25-feature extraction enabled`);

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

interface OHLCV {
  timestamp: number;
  open: number;
  high: number;
  low: number;
  close: number;
  volume: number;
}

// Adaptive training configuration based on data size
// ‚ö° ULTRA-FAST MODE v2: Minimized for edge function CPU limits (~30-40s total)
function getTrainingConfig(dataSize: number) {
  if (dataSize < 200) {
    return {
      curriculum_stage: 'basic',
      features: 15, // technicals only
      sequence_length: 30,
      episodes: 1, // ‚ö° ULTRA-FAST: 1 episode only
      maxStepsPerEpisode: 30, // ‚ö° ULTRA-FAST: 30 steps max
      enable_action_masking: false,
      enable_structural: false
    };
  } else if (dataSize < 500) {
    return {
      curriculum_stage: 'with_sr',
      features: 22, // technicals + S/R + regime
      sequence_length: 40,
      episodes: 1, // ‚ö° ULTRA-FAST: 1 episode only
      maxStepsPerEpisode: 30, // ‚ö° ULTRA-FAST: 30 steps max
      enable_action_masking: false,
      enable_structural: true
    };
  } else if (dataSize < 600) {
    // Standard full config for medium-large assets
    return {
      curriculum_stage: 'full',
      features: 31, // all features
      sequence_length: 50,
      episodes: 1, // ‚ö° ULTRA-FAST: 1 episode only
      maxStepsPerEpisode: 30, // ‚ö° ULTRA-FAST: 30 steps max
      enable_action_masking: true,
      enable_structural: true
    };
  } else {
    // ‚ö° Minimal config for very large assets (‚â•600 bars) to avoid CPU timeouts
    return {
      curriculum_stage: 'full',
      features: 25, // ‚ö° Reduced: core technicals + key structural (remove some Fib/pivot levels)
      sequence_length: 40, // ‚ö° Reduced from 50
      episodes: 1, // ‚ö° ULTRA-FAST: 1 episode only
      maxStepsPerEpisode: 30, // ‚ö° ULTRA-FAST: 30 steps max
      enable_action_masking: true,
      enable_structural: true // Keep structural but with fewer features
    };
  }
}

// CPU timeout protection (Pro tier: 200s limit)
const MAX_TRAINING_TIME_MS = 120000; // ‚ö° ULTRA-FAST: 120 seconds (Pro tier buffer)

// Data augmentation for small datasets
function augmentData(data: OHLCV[], targetSize: number): OHLCV[] {
  if (data.length >= targetSize) return data;
  
  const augmented = [...data];
  const neededDuplicates = targetSize - data.length;
  
  while (augmented.length < targetSize) {
    const randomBar = data[Math.floor(Math.random() * data.length)];
    const noise = 0.995 + Math.random() * 0.01; // ¬±0.5% noise
    augmented.push({
      timestamp: randomBar.timestamp + (augmented.length * 86400000),
      open: randomBar.open * noise,
      high: randomBar.high * noise,
      low: randomBar.low * noise,
      close: randomBar.close * noise,
      volume: randomBar.volume * (0.9 + Math.random() * 0.2)
    });
  }
  
  // Validate augmentation
  if (augmented.length !== targetSize) {
    throw new Error(`Augmentation failed: expected ${targetSize} bars, got ${augmented.length}`);
  }
  
  console.log(`‚úÖ Augmented ${data.length} bars to ${augmented.length} bars (added ${neededDuplicates})`);
  return augmented;
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const authHeader = req.headers.get('Authorization');
    if (!authHeader) {
      return new Response(
        JSON.stringify({ error: 'No authorization header provided' }),
        { status: 401, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const token = authHeader.replace('Bearer ', '');
    const serviceRoleKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY');
    const isServiceRole = token === serviceRoleKey;
    
    let userId: string;
    let supabaseClient;
    let requestBody;
    
    if (isServiceRole) {
      console.log('üîë Service role authentication detected');
      supabaseClient = createClient(
        Deno.env.get('SUPABASE_URL') ?? '',
        serviceRoleKey ?? '',
        {
          auth: {
            autoRefreshToken: false,
            persistSession: false
          }
        }
      );
      
      requestBody = await req.json();
      userId = requestBody.user_id;
      
      if (!userId) {
        return new Response(
          JSON.stringify({ error: 'user_id required for service role calls' }),
          { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }
      
      console.log(`Service role training for user: ${userId}`);
    } else {
      supabaseClient = createClient(
        Deno.env.get('SUPABASE_URL') ?? '',
        Deno.env.get('SUPABASE_ANON_KEY') ?? '',
        { 
          global: { 
            headers: { Authorization: authHeader }
          }
        }
      );
      
      const { data: { user }, error: authError } = await supabaseClient.auth.getUser(token);
      
      if (authError || !user) {
        console.error('Authentication error:', authError);
        return new Response(
          JSON.stringify({ error: 'Authentication failed. Please log in again.' }),
          { status: 401, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }
      
      userId = user.id;
      console.log(`User ${userId} requesting training for symbol`);
      requestBody = await req.json();
    }

    let validatedData;
    try {
      validatedData = validateInput(TrainingRequestSchema, requestBody);
    } catch (error) {
      return createValidationErrorResponse(error as Error, corsHeaders);
    }

    const { symbol: normalizedSymbol, forceRetrain } = validatedData;
    const useAugmentation = requestBody.use_augmentation || false;
    console.log(`Training model for asset: ${normalizedSymbol} (forceRetrain: ${forceRetrain}, augmentation: ${useAugmentation})`);

    // Check if model already exists
    const { data: existingModel, error: checkError } = await supabaseClient
      .from('asset_models')
      .select('id, created_at, performance_metrics')
      .eq('user_id', userId)
      .eq('symbol', normalizedSymbol)
      .maybeSingle();

    if (existingModel && !forceRetrain) {
      console.log(`‚è≠Ô∏è Model already exists for ${normalizedSymbol}, skipping`);
      return new Response(
        JSON.stringify({
          success: true,
          skipped: true,
          symbol: normalizedSymbol,
          message: `Model already exists for ${normalizedSymbol}`,
          existingModel: {
            id: existingModel.id,
            createdAt: existingModel.created_at,
            metrics: existingModel.performance_metrics
          }
        }),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    if (existingModel && forceRetrain) {
      console.log(`üîÑ Force retraining ${normalizedSymbol} - deleting old model`);
      await supabaseClient
        .from('asset_models')
        .delete()
        .eq('id', existingModel.id);
    }

    // Fetch historical data
    const historicalData = await fetchHistoricalData(normalizedSymbol);
    
    // Auto-enable augmentation for datasets with 30-200 bars
    const shouldAugment = useAugmentation || (historicalData.length >= 30 && historicalData.length < 200);
    const minDataPoints = shouldAugment ? 30 : 50;
    
    if (historicalData.length < minDataPoints) {
      throw new Error(`Insufficient data for ${normalizedSymbol}. Need at least ${minDataPoints} data points, got ${historicalData.length}.`);
    }

    let trainingData = historicalData;
    const config = getTrainingConfig(historicalData.length);
    
    // Apply data augmentation if needed
    if (shouldAugment && historicalData.length < 200) {
      const targetSize = Math.min(200, historicalData.length * 3);
      console.log(`üìä Auto-augmenting data from ${historicalData.length} to ${targetSize} bars`);
      trainingData = augmentData(historicalData, targetSize);
    }

    console.log(`üìã Training config: ${config.curriculum_stage} stage, ${config.features} features, ${config.episodes} episodes`);

    // Determine asset type
    const assetType = normalizedSymbol.includes('USD') || normalizedSymbol.includes('-USD') 
      ? 'crypto' 
      : 'stock';

    // Split data for training and testing
    const splitIndex = Math.floor(trainingData.length * 0.8);
    const trainData = trainingData.slice(0, splitIndex);
    const testData = trainingData.slice(splitIndex);

    // Train comprehensive PPO model
    const result = await trainComprehensivePPO(
      normalizedSymbol,
      trainData,
      testData,
      config,
      supabaseClient
    );

    // Check for existing model and determine version
    const { data: activeModel } = await supabaseClient
      .from('asset_models')
      .select('id, model_version, model_storage_path')
      .eq('user_id', userId)
      .eq('symbol', normalizedSymbol)
      .eq('model_status', 'active')
      .single();

    const newVersion = activeModel ? activeModel.model_version + 1 : 1;

    // Generate storage paths
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const assetFolder = `${userId}/${assetType}/${normalizedSymbol}`;
    const modelFileName = `v${newVersion}_${timestamp}.json`;
    const metadataFileName = `v${newVersion}_${timestamp}.meta`;
    const modelPath = `${assetFolder}/${modelFileName}`;
    const metadataPath = `${assetFolder}/${metadataFileName}`;

    console.log(`üìÅ Uploading model to storage: ${modelPath}`);

    // Upload model weights to storage
    const modelBlob = new Blob(
      [JSON.stringify(result.model_weights, null, 2)], 
      { type: 'application/json' }
    );
    const { error: uploadError } = await supabaseClient
      .storage
      .from('trained-models')
      .upload(modelPath, modelBlob, {
        contentType: 'application/json',
        upsert: false
      });

    if (uploadError) {
      console.error('‚ùå Failed to upload model:', uploadError);
      throw new Error(`Storage upload failed: ${uploadError.message}`);
    }

    // Upload training metadata
    const metadataBlob = new Blob(
      [JSON.stringify({
        symbol: normalizedSymbol,
        asset_type: assetType,
        version: newVersion,
        training_config: config,
        training_logs: result.training_logs,
        structural_metadata: result.structural_metadata,
        performance_metrics: result.performance_metrics,
        timestamp: new Date().toISOString()
      }, null, 2)],
      { type: 'application/json' }
    );

    await supabaseClient
      .storage
      .from('trained-models')
      .upload(metadataPath, metadataBlob, {
        contentType: 'application/json',
        upsert: false
      });

    console.log(`‚úÖ Model uploaded successfully: ${modelPath}`);

    // Archive old model if exists
    if (activeModel) {
      console.log(`üì¶ Archiving old model v${activeModel.model_version}`);
      await supabaseClient
        .from('asset_models')
        .update({ model_status: 'archived' })
        .eq('id', activeModel.id);
    }

    // Save metadata to database (without model_weights)
    const { data: insertedModel, error: insertError } = await supabaseClient
      .from('asset_models')
      .insert({
        user_id: userId,
        symbol: normalizedSymbol,
        model_type: 'recurrent_ppo',
        model_architecture: 'recurrent_ppo',
        model_storage_path: modelPath,
        metadata_storage_path: metadataPath,
        model_version: newVersion,
        model_status: 'pending_validation', // üîÑ Mark as pending validation
        action_space: {
          direction: 3,
          tp_offset: [-0.5, 0.5],
          sl_tight: [0.5, 2.0],
          size: [0.0, 1.0]
        },
        hidden_size: 128,
        sequence_length: config.sequence_length,
        curriculum_stage: config.curriculum_stage,
        training_data_points: trainingData.length,
        structural_features: result.structural_metadata,
        performance_metrics: result.performance_metrics,
        fine_tuning_metadata: {
          asset_type: assetType,
          data_augmented: shouldAugment && historicalData.length < 200,
          augmentation_ratio: shouldAugment ? trainingData.length / historicalData.length : 1,
          original_data_points: historicalData.length,
          train_size: trainData.length,
          test_size: testData.length,
          episodes_trained: config.episodes
        }
      })
      .select()
      .single();

    if (insertError) {
      throw insertError;
    }

    console.log(`‚úÖ Model v${newVersion} saved successfully for ${normalizedSymbol}`);
    console.log('üîÑ Model marked as "pending_validation" - validation will run via cron job');
    console.log('‚è≠Ô∏è Skipping inline validation (split training/validation architecture)');
    
    let validationTriggered = false;
    let validationApproved = false;

    return new Response(
      JSON.stringify({
        success: true,
        symbol: normalizedSymbol,
        assetType,
        curriculum_stage: config.curriculum_stage,
        dataPoints: trainingData.length,
        metrics: result.performance_metrics,
        model_id: insertedModel.id,
        validation_triggered: validationTriggered,
        validation_approved: validationApproved
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );

  } catch (error) {
    console.error('Error training asset model:', error);
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
});

async function fetchHistoricalData(symbol: string): Promise<OHLCV[]> {
  console.log(`üì° Fetching training data for ${symbol}...`);
  
  try {
    const data = await fetchUnifiedData({
      symbol,
      range: '2y',
      interval: '1d'
    });
    
    return data.map((d: MarketDataPoint) => ({
      timestamp: d.timestamp,
      open: d.open,
      high: d.high,
      low: d.low,
      close: d.close,
      volume: d.volume
    }));
  } catch (error) {
    console.error(`‚ùå Error fetching training data:`, error);
    throw error;
  }
}

async function trainComprehensivePPO(
  symbol: string,
  trainData: OHLCV[],
  testData: OHLCV[],
  config: any,
  supabaseClient: any
) {
  // Initialize recurrent PPO model
  const model = initializeModel(config.features, 128, config.sequence_length);
  
  // Create trading environment with domain randomization and feature config
  const env = new TradingEnvironment(
    trainData,
    {
      initialBalance: 100000,
      maxPositions: 1,
      feesRange: [0.0004, 0.001],
      slippageRange: [0.0001, 0.0005],
      enableActionMasking: config.enable_action_masking,
      enableStructuralFeatures: config.enable_structural,
      sequenceLength: config.sequence_length
    },
    { features: config.features, enableStructural: config.enable_structural }
  );
  
  // Initialize PPO trainer
  // ‚ö° ULTRA-FAST: Minimal batch size and single epoch
  const trainer = new PPOTrainer(model, {
    gamma: 0.99,
    gae_lambda: 0.95,
    clip_epsilon: 0.2,
    entropy_coef: 0.01,
    learningRate: 0.001, // ‚ö° Higher for faster convergence (was 3e-4)
    batchSize: 8, // ‚ö° Reduced from 16 (75% fewer gradient updates)
    epochs: 1 // ‚ö° Reduced from 2 (50% fewer epochs)
  });
  
  // Training metrics
  const metrics = {
    episodes: [],
    longTrades: 0,
    shortTrades: 0,
    longWins: 0,
    shortWins: 0,
    confluenceScores: [],
    fibAlignments: [],
    tpDistances: [],
    slDistances: []
  };
  
  // Training loop with comprehensive logging and timeout protection
  console.log(`üéØ Starting training: ${config.episodes} episodes`);
  const startTime = Date.now();
  
  for (let episode = 0; episode < config.episodes; episode++) {
    // ‚úÖ TIMEOUT CHECK - prevent edge function CPU timeout
    const elapsed = Date.now() - startTime;
    if (elapsed > MAX_TRAINING_TIME_MS) {
      console.log(`‚è∞ Training timeout after ${episode} episodes (${(elapsed/1000).toFixed(1)}s). Stopping early.`);
      break;
    }
    
    const buffer = trainer.createBuffer();
    let sequenceFeatures = env.reset();
    let done = false;
    let episodeReward = 0;
    let episodeTrades = 0;
    let steps = 0;
    
    while (!done) {
      // ‚ö° ULTRA-FAST: Early termination at 50 steps
      if (steps >= (config.maxStepsPerEpisode || 50)) {
        console.log(`Episode ${episode}: Max steps reached (${steps})`);
        done = true;
        break;
      }
      
      // Bounds check before stepping
      if (env['state'].currentBar >= trainData.length - 1) {
        console.log(`Episode ${episode}: Reached end of data at bar ${env['state'].currentBar}`);
        done = true;
        break;
      }
      
      // ‚úÖ FIX: Change logProb to logProbs (interface mismatch)
      const { action, value, logProbs } = forwardPass(model, sequenceFeatures, false);
      const { nextState, reward, done: isDone, info } = env.step(action);
      
      // ‚úÖ FIX: Compute nextValue for GAE
      const nextValueResult = isDone ? 0 : forwardPass(model, nextState, true).value;
      
      buffer.store({
        state: sequenceFeatures,
        action,
        reward,
        value,
        logProbs, // ‚úÖ FIX: Use logProbs (plural)
        done: isDone,
        nextState, // ‚úÖ FIX: Add nextState
        nextValue: nextValueResult // ‚úÖ FIX: Add nextValue
      });
      
      // ‚úÖ FIX: Correct info property access (info.trade instead of info.trade_executed)
      if (info.trade) {
        episodeTrades++;
        const trade = info.trade;
        
        if (trade.direction === 'long') metrics.longTrades++;
        else if (trade.direction === 'short') metrics.shortTrades++;
        
        if (trade.confluenceScore) metrics.confluenceScores.push(trade.confluenceScore);
        if (trade.fibAlignment) metrics.fibAlignments.push(trade.fibAlignment);
        if (trade.tpDistance) metrics.tpDistances.push(trade.tpDistance);
        if (trade.slDistance) metrics.slDistances.push(trade.slDistance);
        
        // Track wins (trade is only returned when closed)
        if (trade.pnl > 0) {
          if (trade.direction === 'long') metrics.longWins++;
          else metrics.shortWins++;
        }
      }
      
      sequenceFeatures = nextState;
      episodeReward += reward;
      done = isDone;
      steps++;
      
      // ‚úÖ ADD: Step-level logging every 50 steps
      if (steps % 50 === 0) {
        const elapsedSeconds = ((Date.now() - startTime) / 1000).toFixed(1);
        console.log(`  Episode ${episode}/${config.episodes}, Step ${steps}: reward=${episodeReward.toFixed(2)}, equity=${info.equity?.toFixed(0) || 'N/A'} (${elapsedSeconds}s)`);
      }
    }
    
    // Calculate advantages
    const advantages = trainer.computeGAE(buffer);
    buffer.setAdvantages(advantages);
    
    // ‚ö° ULTRA-FAST: Update policy every episode (immediate learning)
    const updateStart = Date.now();
    const trainingMetrics = trainer.updateModel(buffer, advantages);
    const updateTime = ((Date.now() - updateStart) / 1000).toFixed(2);
    const totalElapsed = ((Date.now() - startTime) / 1000).toFixed(1);
    const avgTimePerEpisode = ((Date.now() - startTime) / (episode + 1) / 1000).toFixed(2);
    const estimatedRemaining = (parseFloat(avgTimePerEpisode) * (config.episodes - episode - 1)).toFixed(1);
    
    console.log(`‚úÖ Episode ${episode + 1}/${config.episodes}: reward=${episodeReward.toFixed(2)}, trades=${episodeTrades}, update=${updateTime}s, elapsed=${totalElapsed}s, ETA=${estimatedRemaining}s`);
    
    metrics.episodes.push({
      episode,
      reward: episodeReward,
      pnl: env.getMetrics().total_reward, // ‚úÖ FIX: Use correct property name
      trades: episodeTrades
    });
  }
  
  const totalTime = ((Date.now() - startTime) / 1000 / 60).toFixed(2);
  console.log(`üéâ Training completed in ${totalTime} minutes`);
  
  // Calculate final metrics
  const envMetrics = env.getMetrics();
  const totalTrades = metrics.longTrades + metrics.shortTrades;
  
  // ‚úÖ FIX: Calculate payoff ratios from env metrics
  const longPayoffRatio = envMetrics.long_trades > 0 
    ? Math.abs(envMetrics.avg_win / (envMetrics.avg_loss || -1))
    : 0;
  const shortPayoffRatio = envMetrics.short_trades > 0
    ? Math.abs(envMetrics.avg_win / (envMetrics.avg_loss || -1))
    : 0;
  
  return {
    model_weights: serializeModel(model),
    performance_metrics: {
      totalTrades,
      winRate: totalTrades > 0 ? (metrics.longWins + metrics.shortWins) / totalTrades : 0,
      longWinRate: metrics.longTrades > 0 ? metrics.longWins / metrics.longTrades : 0,
      shortWinRate: metrics.shortTrades > 0 ? metrics.shortWins / metrics.shortTrades : 0,
      longPayoffRatio,
      shortPayoffRatio,
      sharpeRatio: envMetrics.sharpe_ratio || 0, // ‚úÖ FIX: Use correct property name
      maxDrawdown: envMetrics.max_drawdown || 0, // ‚úÖ FIX: Use correct property name
      totalReturn: (envMetrics.final_equity - 100000) / 100000, // ‚úÖ FIX: Calculate from equity
      avgConfluence: metrics.confluenceScores.length > 0 
        ? metrics.confluenceScores.reduce((a, b) => a + b, 0) / metrics.confluenceScores.length 
        : 0,
      fibAlignmentRatio: metrics.fibAlignments.length > 0
        ? metrics.fibAlignments.filter(f => f > 0.8).length / metrics.fibAlignments.length
        : 0,
      avgTPDistanceATR: metrics.tpDistances.length > 0
        ? metrics.tpDistances.reduce((a, b) => a + b, 0) / metrics.tpDistances.length
        : 0,
      avgSLDistanceATR: metrics.slDistances.length > 0
        ? metrics.slDistances.reduce((a, b) => a + b, 0) / metrics.slDistances.length
        : 0
    },
    structural_metadata: {
      confluence_avg: envMetrics.confluence_avg || 0, // ‚úÖ FIX: Use correct property names
      fib_alignment_avg: envMetrics.fib_alignment_avg || 0,
      total_trades: envMetrics.total_trades
    }
  };
}
